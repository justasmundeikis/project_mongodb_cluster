[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MongoDb Cluster Project",
    "section": "",
    "text": "This project aims to demonstrate how to set up a MongoDB cluster, which can perform parallel processing through data sharding (a method of distributing data across multiple machines) and ensure data continuity by implementing database replication (copying data across multiple nodes to ensure availability).\n\n\nMongoDB is favored for its built-in capabilities to handle sharding and clustering. This setup allows it to manage large-scale data with high availability and fault tolerance, making it a robust choice for distributed data systems. Even though AIS data is structured and could fit naturally into SQL databases like DuckDB, this project’s focus is to explore MongoDB’s unique features and capabilities in handling distributed data schemes.\nComparatively, MongoDB’s approach is more straightforward than configuring MySQL/MariaDB with Vitess, which requires additional tooling for scaling. Looking further, technologies like CockroachDB, a distributed SQL database offering sharding and replication, present interesting avenues for exploration in the field of distributed databases.\n\n\n\n\nDocker and Docker Compose Setup:\n\n\nWe’ll start by setting up Docker, a platform that simplifies the deployment of applications using containers. With Docker Compose, you’ll create multiple Docker containers running MongoDB instances and connect them through Docker’s bridge network. This network provides isolated communication between containers, simulating a real-world database cluster environment.\n\n\nInitializing MongoDB:\n\n\nYou’ll learn to initialize and configure the MongoDB instances. We’ll walk through setting up sharding, which allows efficient data distribution, and creating indexes, which enhance query performance by reducing data retrieval time.\n\n\nHandling AIS Data:\n\n\nThe project downloads and processes AIS (Automatic Identification System) data. AIS is a vital maritime technology for tracking vessels. We’ll utilize Polars, a high-performance DataFrame library optimized for speed and efficiency, to clean and prepare the data. This cleaned data will then be ingested into the MongoDB cluster for further analysis.\n\n\n\n\n\nMongoDB instances can heavily utilize RAM (Random Access Memory) due to their nature of holding data temporarily in memory before committing it to storage. Effective RAM management is crucial to maintain system performance, especially as data volumes grow.\n\n\n\n\n\nThis project is built in Python, and we provide a requirements.txt file to facilitate setting up the same environment on any machine. This file contains all the necessary libraries and their versions, ensuring that anyone following the guide can replicate the setup and results seamlessly."
  },
  {
    "objectID": "index.html#why-use-a-mongodb-cluster",
    "href": "index.html#why-use-a-mongodb-cluster",
    "title": "MongoDb Cluster Project",
    "section": "",
    "text": "MongoDB is favored for its built-in capabilities to handle sharding and clustering. This setup allows it to manage large-scale data with high availability and fault tolerance, making it a robust choice for distributed data systems. Even though AIS data is structured and could fit naturally into SQL databases like DuckDB, this project’s focus is to explore MongoDB’s unique features and capabilities in handling distributed data schemes.\nComparatively, MongoDB’s approach is more straightforward than configuring MySQL/MariaDB with Vitess, which requires additional tooling for scaling. Looking further, technologies like CockroachDB, a distributed SQL database offering sharding and replication, present interesting avenues for exploration in the field of distributed databases."
  },
  {
    "objectID": "index.html#key-objectives",
    "href": "index.html#key-objectives",
    "title": "MongoDb Cluster Project",
    "section": "",
    "text": "Docker and Docker Compose Setup:\n\n\nWe’ll start by setting up Docker, a platform that simplifies the deployment of applications using containers. With Docker Compose, you’ll create multiple Docker containers running MongoDB instances and connect them through Docker’s bridge network. This network provides isolated communication between containers, simulating a real-world database cluster environment.\n\n\nInitializing MongoDB:\n\n\nYou’ll learn to initialize and configure the MongoDB instances. We’ll walk through setting up sharding, which allows efficient data distribution, and creating indexes, which enhance query performance by reducing data retrieval time.\n\n\nHandling AIS Data:\n\n\nThe project downloads and processes AIS (Automatic Identification System) data. AIS is a vital maritime technology for tracking vessels. We’ll utilize Polars, a high-performance DataFrame library optimized for speed and efficiency, to clean and prepare the data. This cleaned data will then be ingested into the MongoDB cluster for further analysis."
  },
  {
    "objectID": "index.html#considerations",
    "href": "index.html#considerations",
    "title": "MongoDb Cluster Project",
    "section": "",
    "text": "MongoDB instances can heavily utilize RAM (Random Access Memory) due to their nature of holding data temporarily in memory before committing it to storage. Effective RAM management is crucial to maintain system performance, especially as data volumes grow."
  },
  {
    "objectID": "index.html#ensuring-reproducibility",
    "href": "index.html#ensuring-reproducibility",
    "title": "MongoDb Cluster Project",
    "section": "",
    "text": "This project is built in Python, and we provide a requirements.txt file to facilitate setting up the same environment on any machine. This file contains all the necessary libraries and their versions, ensuring that anyone following the guide can replicate the setup and results seamlessly."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "MongoDb Cluster Project",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is Docker?\nDocker is a platform for containerization, which represents a lightweight form of virtualization. It allows you to package applications and their dependencies into standardized units, called containers. These containers can run consistently across different computing environments, enhancing reproducibility and streamlining the deployment process. This capability enables users to replicate work environments seamlessly and maintain consistent software operations from development to production.\n\n\nWhy Docker Performs Better on Linux\nDocker’s architecture leverages Linux kernel features such as namespaces and cgroups. These features allow Docker to run containers with minimal overhead by directly interacting with the kernel. As a result, Docker achieves optimal performance on Linux, benefiting from its native containerization support. On MacOS and Windows, Docker operates by running a Linux virtual machine to emulate these capabilities, which introduces additional overhead and may lead to less efficient use of system resources compared to native Linux environments.\n\n\nLearning Resources\nFor more detailed learning, explore these helpful Docker resources: [Add URL1 here] and [Add URL2 here]."
  },
  {
    "objectID": "index.html#installing-docker",
    "href": "index.html#installing-docker",
    "title": "MongoDb Cluster Project",
    "section": "Installing Docker",
    "text": "Installing Docker\nTo install Docker, visit the Docker website for the official installation instructions. This includes steps for installing Docker, Docker Compose, and for MacOS or Windows users, Docker Desktop. However, since this documentation primarily targets Linux users due to Docker’s native compatibility with Linux, the instructions provided will focus on terminal commands relevant to Linux systems."
  },
  {
    "objectID": "index.html#what-is-docker-compose",
    "href": "index.html#what-is-docker-compose",
    "title": "MongoDb Cluster Project",
    "section": "What is Docker Compose?",
    "text": "What is Docker Compose?\nDocker Compose is a tool that simplifies the configuration and management of multi-container Docker applications. It utilizes a YAML file, typically named docker-compose.yml, to define services, networks, and volumes required for an application. This central configuration allows you to launch an entire multi-service application with one command, significantly enhancing productivity and simplifying the management of complex environments.\n\nKey Docker Compose Commands\n\nStarting Services:\n\ndocker compose up -d: Initializes services as defined in the docker-compose.yml file. The -d flag runs the containers in detached mode, meaning they execute in the background, and your terminal remains available for other commands.\ndocker compose up -d --build --pull: Includes the --build flag to rebuild specified build contexts and the --pull flag to ensure services use the latest images from the registry.\n\nListing Running Containers:\n\ndocker ps: Displays all currently running Docker containers, showing service names, container IDs, ports, and image names—tools essential for monitoring active services.\n\nStopping Services:\n\ndocker compose down: Halts all running services defined by Docker Compose.\ndocker compose down --rmi all --volumes: Removes associated resources such as images and volumes, effectively cleaning up your environment.\nStopping Individual Containers: Use docker stop &lt;container_id&gt; to stop a specific container, which is useful for testing fault resilience. For instance, you can simulate primary MongoDB instance failures by selectively bringing down containers.\n\nRestarting Services:\n\ndocker compose restart: Restarts all services as specified in your docker-compose.yml file.\n\n\nBy mastering these commands and understanding their applications, you’ll streamline container management, enhance operational efficiency, and maintain robust control over your Dockerized applications."
  },
  {
    "objectID": "index.html#sharding",
    "href": "index.html#sharding",
    "title": "MongoDb Cluster Project",
    "section": "Sharding",
    "text": "Sharding\nSharding is a strategy used in MongoDB to distribute data across multiple servers (or database instances) to efficiently manage large datasets. It works by dividing the dataset into smaller, easily manageable segments called “shards,” based on a shard key. The shard key acts as an index that determines how data is split and stored across different shards. For instance, if you’re working with data from 2020 to 2025, you could shard the data by year, allowing the system to query data for a specific year by accessing only the relevant shard. This optimizes read operations by minimizing unnecessary data scans."
  },
  {
    "objectID": "index.html#sharding-vs.-partitioning",
    "href": "index.html#sharding-vs.-partitioning",
    "title": "MongoDb Cluster Project",
    "section": "Sharding vs. Partitioning",
    "text": "Sharding vs. Partitioning\nSharding: This is the process of distributing data across multiple servers or nodes, enabling the system to scale horizontally. It helps in managing very large datasets by dividing them into shards, with each residing on a different server. This setup allows for parallel processing and efficient load balancing, especially when a single server is not sufficient to handle the data volume.\nPartitioning: Unlike sharding, partitioning generally refers to breaking down a database table into smaller segments within a single database instance. This method is typically employed to enhance performance and manageability on a smaller scale. However, MongoDB does not support partitioning as seen in traditional relational databases, relying instead on sharding for distributing large datasets across multiple nodes."
  },
  {
    "objectID": "index.html#role-of-indexing",
    "href": "index.html#role-of-indexing",
    "title": "MongoDb Cluster Project",
    "section": "Role of Indexing",
    "text": "Role of Indexing\nIndexing is critical in both sharded and non-sharded environments as it improves data retrieval speed. An index in MongoDB is a data structure (like a B-tree or hash) that helps in quick searches and reduces query execution time. Indexing allows the database to locate specific records swiftly without scanning entire datasets, which is especially crucial in large-scale applications.\nIn a Sharded Environment: Each shard maintains its own indexes to optimize data access locally. The combination of effective sharding and indexing ensures that queries are efficiently routed to the correct shard, minimizing inter-shard operations."
  },
  {
    "objectID": "index.html#combining-sharding-and-indexing-in-mongodb",
    "href": "index.html#combining-sharding-and-indexing-in-mongodb",
    "title": "MongoDb Cluster Project",
    "section": "Combining Sharding and Indexing in MongoDB",
    "text": "Combining Sharding and Indexing in MongoDB\nThough MongoDB doesn’t employ traditional partitioning, it combines sharding and indexing to optimize data storage, retrieval, and management:\nChoosing a Shard Key: The shard key is pivotal as it dictates the distribution of data across shards. A well-chosen shard key aligns with prevalent query patterns to maximize efficiency. In cases of unclear distribution, MongoDB can employ hashed sharding, distributing data evenly using a hash of the shard key.\nImplementing Indexing: Deploying indexes on frequently queried fields, such as timestamps or identifiers (like MMSI numbers), expedites data retrievals across extensive datasets."
  },
  {
    "objectID": "index.html#replication",
    "href": "index.html#replication",
    "title": "MongoDb Cluster Project",
    "section": "Replication",
    "text": "Replication\nReplication in MongoDB is designed to ensure data reliability and fault tolerance. It involves maintaining copies of the same dataset on multiple servers, known as a “replica set.” If one server fails (e.g., due to hardware issues like a disk failure or resource exhaustion), another replica can take over, ensuring continuous data availability.\nIn a replica set, one server acts as the “primary,” receiving all write operations. The others are “secondaries” that replicate the primary’s data asynchronously. This setup enhances reliability by allowing the system to continue operating even if a primary fails, as a secondary can be promoted to primary.\nIn this project, we will configure 2 shards, each with one primary and 2 secondary replicas for redundancy."
  },
  {
    "objectID": "index.html#orchestration",
    "href": "index.html#orchestration",
    "title": "MongoDb Cluster Project",
    "section": "Orchestration",
    "text": "Orchestration\nManaging multiple shards and replicas requires careful orchestration to ensure seamless operation:\n\nMongoDB Router (mongos): This acts as the client interface, routing queries and operations to the appropriate shards based on the chosen shard key.\nConfig Servers: These hold metadata and configuration details for the cluster, such as data distributions among the shards. It’s crucial to have an odd number of config servers to achieve a voting quorum, ensuring fault tolerance during decision-making and maintaining cluster integrity. Commonly, three config servers are recommended."
  },
  {
    "objectID": "index.html#importing-data",
    "href": "index.html#importing-data",
    "title": "MongoDb Cluster Project",
    "section": "Importing data",
    "text": "Importing data\nWe will source our data from the AIS Data Source by downloading the following datasets for analysis:\n\naisdk-2025-05-01.zip\naisdk-2025-05-02.zip\naisdk-2025-05-03.zip\naisdk-2025-05-04.zip\naisdk-2025-05-05.zip\naisdk-2025-05-06.zip\naisdk-2025-05-07.zip\n\nTo manage the downloading of these datasets, we use a Python script, download_raw_data.py, that performs these tasks:\n\nDownloads each specified dataset.\nExtracts the contents from the downloaded ZIP files.\nDeletes the ZIP files once extraction is complete.\nUtilizes ThreadPoolExecutor to handle tasks concurrently, significantly reducing the time required for downloading, which is typically an I/O-bound and time-consuming task.\n\nimport pathlib\nimport urllib.parse\nimport requests\nimport zipfile\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Define a local path under your current working directory\nimported_data_dir = pathlib.Path.cwd() / \"imported_data\"\n\n# Create the directory if it doesn't exist\nimported_data_dir.mkdir(parents=True, exist_ok=True)\n\nurl = \"https://web.ais.dk/aisdata/\"\nfiles = [\n    'aisdk-2025-05-01.zip',\n    'aisdk-2025-05-02.zip',\n    'aisdk-2025-05-03.zip',\n    'aisdk-2025-05-04.zip',\n    'aisdk-2025-05-05.zip',\n    'aisdk-2025-05-06.zip',\n    'aisdk-2025-05-07.zip',\n]\n\ndef download_file(file_name, base_url, destination_dir):\n    \"\"\"Download a single zip file.\"\"\"\n    full_url = urllib.parse.urljoin(base_url, file_name)\n    zip_path = destination_dir / file_name\n    try:\n        print(f\"Downloading: {file_name} from {full_url}\")\n        response = requests.get(full_url, timeout=10)  # Added timeout for debugging\n        response.raise_for_status()\n        with open(zip_path, 'wb') as file:\n            file.write(response.content)\n        print(f\"Download complete: {file_name}\")\n    except requests.RequestException as e:\n        print(f\"Failed to download {file_name}: {e}\")\n\ndef extract_file(zip_path):\n    \"\"\"Extract a single zip file.\"\"\"\n    if zip_path.exists():\n        print(f\"Extracting: {zip_path.name}\")\n        try:\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(zip_path.parent)\n            print(f\"Extraction complete: {zip_path.name}\")\n        except zipfile.BadZipFile:\n            print(f\"Failed to extract {zip_path.name}: Bad zip file\")\n\ndef delete_file(zip_path):\n    \"\"\"Delete a single zip file.\"\"\"\n    if zip_path.exists():\n        print(f\"Deleting: {zip_path.name}\")\n        try:\n            zip_path.unlink()\n            print(f\"Deletion complete: {zip_path.name}\")\n        except Exception as e:\n            print(f\"Failed to delete {zip_path.name}: {e}\")\n\n# A reasonable number of workers given the I/O-bound nature\nmax_workers = 16\n\n\nprint(\"Starting downloads...\")\nwith ThreadPoolExecutor(max_workers=max_workers) as executor:\n    executor.map(lambda f: download_file(f, url, imported_data_dir), files)\n\nzip_files = list(imported_data_dir.glob(\"*.zip\"))\n\nprint(\"Starting extraction...\")\nwith ThreadPoolExecutor(max_workers=max_workers) as executor:\n    executor.map(extract_file, zip_files)\n\nprint(\"Starting deletion...\")\nwith ThreadPoolExecutor(max_workers=max_workers) as executor:\n    executor.map(delete_file, zip_files)\n\nprint(\"Process complete.\")\nFuture Improvements\nTo enhance this process, consider implementing a method to automatically download the most recent datasets available on the source website. This could involve:\n\nDynamically determining the current date range and downloading most recent datasets.\nCrawling the website to capture the latest available file entries and utilizing these URLs for download.\n\nThese improvements could ensure you’re always working with the most up-to-date datasets, removing the need to manually specify file names."
  },
  {
    "objectID": "index.html#data-cleaning",
    "href": "index.html#data-cleaning",
    "title": "MongoDb Cluster Project",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe clean_data.py script is responsible for processing raw data files. It executes the following tasks:\n\nLoads all .csv files located in the imported_data directory.\nCleans up the column names by standardizing them to lower case and replacing spaces with underscores.\nDrops duplicate rows, where uniqueness is defined by a combination of the timestamp and mmsi columns.\nTransforms the timestamp format from dd/mm/yyyy to yyyy-mm-dd.\nSaves the cleaned data as Parquet files in the clean_data directory, which is efficient for storage and query performance.\n\nData cleaning can be resource-intensive. Ensure your system has at least 32 GB of RAM. Configuring a swap file of at least 12 GB is recommended to maintain system stability during large data operations.\nimport pathlib\nimport polars as pl\nimport gc\n\n# Create cleaned data directory\nimport_data_path = pathlib.Path.cwd() / \"imported_data\"\ncleaned_data_path = pathlib.Path.cwd() / \"clean_data\"\n\n# Ensure cleaned_data directory exists\ncleaned_data_path.mkdir(parents=True, exist_ok=True)\n\n# List all CSV files in the imported_data directory\ncsv_files = list(import_data_path.glob(\"*.csv\"))\n\nfor csv_file in csv_files:\n    print(f\"Processing {csv_file.name}\")\n\n    df = pl.read_csv(csv_file)\n    df = df.rename({\"# Timestamp\": \"timestamp\"})\n    new_column_names = {col: col.lower().replace(' ', '_') for col in df.columns}\n    df = df.rename(new_column_names)\n\n    # Clean and parse Timestamp\n    df = df.with_columns(\n        pl.col(\"timestamp\")\n        #.cast(str)\n        #.str.strip_chars()\n        .str.strptime(pl.Datetime, format=\"%d/%m/%Y %H:%M:%S\", strict=False)\n    )\n\n    # Add 'date' column\n    df = df.with_columns(\n        pl.col(\"timestamp\").dt.date().alias(\"date\")\n    )\n\n    # Format Timestamp to ISO string (before saving)\n    df = df.with_columns(\n        pl.col(\"timestamp\").dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n    )\n\n    df = df.unique(subset=[\"timestamp\", \"mmsi\"], maintain_order=True)\n    cleaned_filename = csv_file.stem + '-cleaned.parquet'\n    df.write_parquet(cleaned_data_path / cleaned_filename)\n    print(f\"Saved cleaned file as {cleaned_filename}\")\n\n    # Deleteing Dataframe and forcein garbace collection\n    del df\n    gc.collect()\n\nprint(\"Process complete\")"
  },
  {
    "objectID": "index.html#docker-compose.yml",
    "href": "index.html#docker-compose.yml",
    "title": "MongoDb Cluster Project",
    "section": "docker-compose.yml",
    "text": "docker-compose.yml\nThe docker-compose.yml file orchestrates multiple MongoDB components necessary for a sharded cluster environment. We’ll look at how to configure config servers, shards, and the router.\n\nConfig Servers Setup\nThe config servers hold metadata and settings for the MongoDB cluster, crucial for managing data across shards.\n\nImage and Containers: We use the mongo:latest image to ensure we’re using the most recent stable MongoDB release for all components. Each config server is named configsvr1, configsvr2, and configsvr3.\nCommand:\n\n--configsvr: Indicates it’s a config server.\n--replSet cfgRS: Specifies the replica set name for the config servers, allowing them to replicate data for redundancy.\n--port 27017: The port on which the server listens.\n--wiredTigerCacheSizeGB 0.5: Limits the WiredTiger cache size to optimize memory usage.\n\nPorts and Volumes:\n\nMaps the host port 27017 to the container’s port 27017, making it accessible.\nBinds ./persistent_data/configsvr1 to /data/db in the container, ensuring data persistence between restarts.\n\nNetworking: Uses the mongo-cluster network, a custom Docker bridge network allowing inter-container communication within our MongoDB cluster.\n\n  configsvr1:\n    image: mongo:latest\n    container_name: configsvr1\n    command: --configsvr --replSet cfgRS --port 27017 --wiredTigerCacheSizeGB 0.5\n    ports:\n      - 27017:27017\n    volumes:\n      - ./persistent_data/configsvr1:/data/db\n    networks:\n      - mongo-cluster\n\n\nShard Setup\nEach shard server is set up similarly, with unique ports and specific configurations.\n\nCommand:\n\n--shardsvr: Specifies that it is a shard server.\n--replSet shard1RS: Defines the replica set for shard nodes for data replication.\n\nPorts and Volumes:\n\nShard 1 runs on port 27018, and Shard 2 runs on port 27019.\nEach shard node binds a local directory for data persistence.\n\n\n  shard1-node1:\n    image: mongo:latest\n    container_name: shard1-node1\n    command: --shardsvr --replSet shard1RS --port 27018 --wiredTigerCacheSizeGB 0.5\n    ports:\n      - 27018:27018\n    volumes:\n      - ./persistent_data/shard1-node1:/data/db\n    networks:\n      - mongo-cluster\n\n\nMongoDB Router (mongos) Setup\nThe mongos router is responsible for directing queries to the appropriate shards.\n\nDependencies: The depends_on option ensures the router starts only after the config servers are up and running.\nPorts and Command:\n\nThe router listens on port 27020 (mapped internally to 27017), ensuring external access.\nCommand:\n\nmongos: Starts the router service.\n--configdb cfgRS/configsvr1:27017,configsvr2:27017,configsvr3:27017: Specifies the config servers to use.\n--bind_ip_all: Allows the container to be accessible externally.\n\n\n\n  mongos:\n    image: mongo:latest\n    container_name: mongos\n    depends_on:\n      - configsvr1\n      - configsvr2\n      - configsvr3\n    ports:\n      - 27020:27017\n    command: &gt;\n      mongos --configdb cfgRS/configsvr1:27017,configsvr2:27017,configsvr3:27017 --bind_ip_all\n    networks:\n      - mongo-cluster\n\n\nNetwork Configuration\n\nBridge Network: The mongo-cluster network uses the bridge driver, which is Docker’s default network type. It allows containers on the same network to communicate with each other while isolating them from other networks.\n\nnetworks:\n  mongo-cluster:\n    driver: bridge\n\n\nLaunching the Setup\nTo start the MongoDB cluster, run the following command in your terminal:\ndocker compose up -d\n\nThis will launch all configured MongoDB instances in detached mode, running the services in the background.\nBy following this setup, you create a robust MongoDB cluster with configuration servers and shards, ensuring data distribution and fault tolerance.\ndocker ps \nShoudl show something similar"
  },
  {
    "objectID": "index.html#initialize-relica-set",
    "href": "index.html#initialize-relica-set",
    "title": "MongoDb Cluster Project",
    "section": "initialize relica set",
    "text": "initialize relica set\nThis script is designed to initialize replica sets for a MongoDB sharded cluster setup. Here’s a concise and detailed breakdown:\n\nInitiate Config Server Replica Set:\n\nThe script begins by initiating the replica set for the config servers, which manage metadata and configuration settings for the sharded cluster.\nIt uses the mongosh shell inside the configsvr1 container to execute the rs.initiate() command.\nThis command defines a replica set named cfgRS, indicating it’s a configuration server replica set with three members (configsvr1, configsvr2, configsvr3), each listening on port 27017.\n\nInitiate Shard 1 Replica Set:\n\nNext, the script sets up the first shard’s replica set.\nAgain, mongosh is used within the shard1-node1 container to initiate shard1RS.\nIt makes shard1-node1, shard1-node2, and shard1-node3 members of this replica set, all listening on port 27018.\n\nInitiate Shard 2 Replica Set:\n\nFinally, the script configures the second shard’s replica set.\nThe initiation command runs in the shard2-node1 container, creating a replica set named shard2RS.\nThe members include shard2-node1, shard2-node2, and shard2-node3, using port 27019.\n\nVerification and Delays:\n\nAfter each initiation, the script checks if the command executed successfully. If an error occurs, the script halts execution.\nBrief pauses (sleep 5) are added after each configuration to ensure the changes have time to propagate throughout the network.\n\n\nIn summary, this script configures a MongoDB environment with distinct replica sets for configuration servers and two separate shard groups, essential steps in implementing a scalable and fault-tolerant sharded cluster.\n#!/bin/bash\n\n# Function to check if a command was successful\nfunction check_status {\n  if [ $? -ne 0 ]; then\n    echo \"❌ An error occurred with the last command. Exiting script.\"\n    exit 1\n  fi\n}\n\n# ---------- Initiate Config Server Replica Set ----------\necho \"⚙️  Initiating Config Server Replica Set...\"\ndocker exec configsvr1 mongosh --port 27017 --eval '\nrs.initiate({\n  _id: \"cfgRS\",\n  configsvr: true,\n  members: [\n    { _id: 0, host: \"configsvr1:27017\" },\n    { _id: 1, host: \"configsvr2:27017\" },\n    { _id: 2, host: \"configsvr3:27017\" }\n  ]\n})'\n\ncheck_status\n\n# Allow some time for the config server setup\nsleep 5\n\n# ---------- Initiate Shard 1 Replica Set ----------\necho \"⚙️  Initiating Shard 1 Replica Set...\"\ndocker exec shard1-node1 mongosh --port 27018 --eval '\nrs.initiate({\n  _id: \"shard1RS\",\n  members: [\n    { _id: 0, host: \"shard1-node1:27018\" },\n    { _id: 1, host: \"shard1-node2:27018\" },\n    { _id: 2, host: \"shard1-node3:27018\" }\n  ]\n})'\n\ncheck_status\n\n# Allow some time for the shard 1 setup\nsleep 5\n\n# ---------- Initiate Shard 2 Replica Set ----------\necho \"⚙️  Initiating Shard 2 Replica Set...\"\ndocker exec shard2-node1 mongosh --port 27019 --eval '\nrs.initiate({\n  _id: \"shard2RS\",\n  members: [\n    { _id: 0, host: \"shard2-node1:27019\" },\n    { _id: 1, host: \"shard2-node2:27019\" },\n    { _id: 2, host: \"shard2-node3:27019\" }\n  ]\n})'\n\ncheck_status\n\necho \"✅ Replica sets initialized!\""
  },
  {
    "objectID": "index.html#adding-sharding",
    "href": "index.html#adding-sharding",
    "title": "MongoDb Cluster Project",
    "section": "adding sharding",
    "text": "adding sharding\nSharding is crucial for scaling databases horizontally, meaning that data is distributed across multiple servers, allowing for high availability and load balancing. This is especially important for large datasets, ensuring efficient resource utilization and performance.\n1 Added Shards to the Cluster:\nFirst, we connect to the MongoDB cluster using the MongoClient. The cluster is accessed through the mongos process, which acts as the point of contact between the application and the MongoDB sharded cluster. It intelligently routes queries to the correct shards based on metadata and sharding keys.\nBy connecting via mongos, we leverage a centralized, intelligent query routing system crucial for scalable data operations.\nclient = MongoClient(\"mongodb://localhost:27020\")\nStep 2: Add Shards to the Cluster\nNext, we define and add our shards. Shards are essentially distributed chunks of your database, each managing a partition of the data. This helps alleviate bottlenecks by spreading data across multiple nodes.\nWe add shards using the addShard command. Each shard typically runs as a replica set—a group of mongod instances that maintain the same data set. These commands tell MongoDB how to distribute your database across multiple machines to optimize for both data redundancy and processing load.\nStep 4: Set Up Indexing\nclient.admin.command(\"addShard\", \"shard1RS/shard1-node1:27018,shard1-node2:27018,shard1-node3:27018\")\nclient.admin.command(\"addShard\", \"shard2RS/shard2-node1:27019,shard2-node2:27019,shard2-node3:27019\")\nStep 4: Set Up Indexing\nBefore we shard a collection, it is crucial to set up an index on the field we intend to use as a shard key. Indexing optimizes both the query performance and the data distribution process.\nHere, we choose to create a hashed index on the MMSI field. A hashed index provides an even distribution of the data across all shards, which is particularly beneficial when the distribution of MMSI values is unknown or uneven. The hashed index is a preferred choice when you anticipate a non-uniform distribution of key values. It effectively minimizes the risk of creating unbalanced shards, which can lead to performance bottlenecks.\nvessel_collection.create_index([(\"MMSI\", \"hashed\")])\nStep 5: Shard the Collection Finally, we shard the vesseldata collection in the ais_db database by the MMSI field using the hashed shard key. This step initiates the distribution of the data across all available shards based on the hash values of the MMSI field. Sharding the collection ensures that as the database grows, data is split intelligently, allowing the cluster to handle increased load without compromising performance.\nclient.admin.command(\"shardCollection\", \"ais_db.vesseldata\", key={\"MMSI\": \"hashed\"})\nits very important to ensure, that the balancer service has started as well.\nclient.admin.command(\"balancerStart\")\nError Handling\nThroughout this process, we include error handling to manage potential issues that might arise during the setup. This ensures robustness in the scripts and provides informative feedback to identify and fix issues quickly.\nWhole initialize_sharding.py\nfrom pymongo import MongoClient, errors\n\n# Connect to the MongoDB sharded cluster via mongos\nclient = MongoClient(\"mongodb://localhost:27020\")  # Connecting to mongos\n\ntry:\n    # Add Shard 1 and Shard 2 to the cluster\n    print(\"⏳ Adding Shard 1...\")\n    client.admin.command(\"addShard\", \"shard1RS/shard1-node1:27018,shard1-node2:27018,shard1-node3:27018\")\n\n    print(\"⏳ Adding Shard 2...\")\n    client.admin.command(\"addShard\", \"shard2RS/shard2-node1:27019,shard2-node2:27019,shard2-node3:27019\")\n\n    # Enable sharding for the ais_db database\n    print(\"⚙️ Enabling Sharding for the ais_db database...\")\n    client.admin.command(\"enableSharding\", \"ais_db\")\n\n    # Create a hashed index on the sharding key before sharding the collection\n    print(\"⚙️ Creating hashed index on 'mmsi' field...\")\n    vessel_collection = client[\"ais_db\"][\"vesseldata\"]\n    vessel_collection.create_index([(\"mmsi\", \"hashed\")])\n\n    # Shard the vesseldata collection by the MMSI field\n    print(\"⚙️ Sharding collection vesseldata by mmsi...\")\n    client.admin.command(\"shardCollection\", \"ais_db.vesseldata\", key={\"mmsi\": \"hashed\"})\n\n    # Start the balancer to redistribute data across shards\n    print(\"🚀 Starting the balancer...\")\n    client.admin.command(\"balancerStart\")\n\n    print(\"✅ Shards added, sharding enabled, and balancer started successfully!\")\n\nexcept errors.OperationFailure as e:\n    print(f\"❌ Operation failed: {e}\")\nexcept Exception as e:\n    print(f\"❌ Unexpected error: {e}\")"
  },
  {
    "objectID": "index.html#inserting-data",
    "href": "index.html#inserting-data",
    "title": "MongoDb Cluster Project",
    "section": "Inserting data",
    "text": "Inserting data\nIn this step we follwo follwoing logic - we read in all .parquet files that are available in the clean_data directiry. We split each data file into chunks of 100 000 rows. We insert each chunk separately.\nUsing the size of 100 000 rows per chunk balances between speed and system resource usage.\nGiven there are 10 instances of mongo db running with 2 shards and each shard has 2 replica set, the CPU is maxed out even in single file (not parrlell processing).\n\nHowever very strange, that during the upload, the ncdu only shows that data is being ingested into shard1\n\nAfter a lengthy period, the job was finished, but the cpu usage was still maxed out at 100% in all cores / threads. Then the redistribution started after the insert. which also took a while. This suggests the importance of not stopping the mondgo instances immidialty after data ingestion, but rather the need to wait until intenral processes are finished.\nIt would be great to have a better monitoring system fro the mongo db cluster\nimport polars as pl\nfrom pymongo import MongoClient\nfrom pathlib import Path\nimport gc\n\n# MongoDB connection setup\nclient = MongoClient(\"mongodb://localhost:27020\")  # Connect to mongos router\ndb = client.ais_db\ncollection = db.vesseldata\n\n# Configuration\nparquet_dir = Path(\"clean_data\")  # Directory containing Parquet files\nchunk_size = 100_000\n\n# Function to insert a chunk of data into MongoDB\ndef insert_chunk(data_chunk, chunk_num):\n    \"\"\"Insert a chunk of data into the MongoDB collection.\"\"\"\n    records = data_chunk.to_dicts()\n    collection.insert_many(records, ordered=False)\n    print(f\"✅ Inserted chunk {chunk_num} ({len(records)} rows)\")\n    del records, data_chunk\n    gc.collect()\n\n# Process all Parquet files\nprint(\"⏳ Starting MongoDB data insertion...\")\n\nfor file_number, file_path in enumerate(parquet_dir.glob(\"*.parquet\"), start=1):\n    print(f\"⏳ Processing file {file_number}: {file_path.name}\")\n    \n    df = pl.read_parquet(file_path)\n    total_rows = df.height\n    chunk_num = 1\n\n    # Insert data in chunks\n    for i in range(0, total_rows, chunk_size):\n        chunk = df.slice(i, chunk_size)\n        insert_chunk(chunk, chunk_num)\n        chunk_num += 1\n\n    print(f\"✅ Completed processing file {file_number}: {file_path.name}\")\n\nprint(\"✅ All data inserted successfully.\")"
  },
  {
    "objectID": "index.html#indexing-collection",
    "href": "index.html#indexing-collection",
    "title": "MongoDb Cluster Project",
    "section": "Indexing collection",
    "text": "Indexing collection\nOnce the data insertion has finished and data was distributed accross shards, now we can insert collection level indexing that should increase the speed of data retrieval. We index on 3 columns:\n\nmmsi\ntimestamp\ndate\n\nAs these are columns to be used most often for selecting an object in the collection.\nfrom pymongo import MongoClient\n\n# Connect to the MongoDB router (mongos)\nclient = MongoClient(\"mongodb://localhost:27020\")  # Make sure the port is correct for your mongos\n\n# Use consistent database and collection names aligned with your setup\n# Assuming consistent naming aligning with previous scripts\ndb = client[\"ais_db\"]  # Change to \"ais_db\" if consistent with your previous setup\ncollection = db[\"vesseldata\"]  # Change to \"vesseldata\" for consistency\n\n# Fields to index for efficient filtering and retrieval\nfields_to_index = [\n    \"timestamp\",\n    \"date\",\n    \"mmsi\"\n]\n\n# Create indexes on these fields\nfor field in fields_to_index:\n    print(f\"📌 Creating index on '{field}'...\")\n    collection.create_index(field)\n\nprint(\"✅ All indexes created.\")"
  },
  {
    "objectID": "REAMDE.html",
    "href": "REAMDE.html",
    "title": "MongoDB Cluster project",
    "section": "",
    "text": "MongoDB Cluster project\nTBD"
  }
]